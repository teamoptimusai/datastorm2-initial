{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imbalanced-structured-data-analysis.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUWkKzPS5OVJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aLTONsKRHk1"
      },
      "source": [
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM1bhgDtZ82G"
      },
      "source": [
        "!gdown --id 1MIKKj8Gi-xUwhsYt6xEV6FSmX0_Le8iL\n",
        "!unzip -q 'data-storm-20.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqKZFScgRKkQ"
      },
      "source": [
        "file = tf.keras.utils\n",
        "raw_df = pd.read_csv('/content/Hotel-A-train.csv', index_col=0)\n",
        "val_df = pd.read_csv('/content/Hotel-A-validation.csv', index_col=0)\n",
        "raw_df = raw_df.append(val_df)\n",
        "raw_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cCU8P6ldV4C"
      },
      "source": [
        "object_cols = ['Gender', 'Ethnicity', 'Educational_Level',\n",
        "       'Income', 'Country_region', 'Hotel_Type', \n",
        "       'Meal_Type', 'Visted_Previously', 'Previous_Cancellations',\n",
        "       'Deposit_type', 'Booking_channel', 'Required_Car_Parking',\n",
        "       'Reservation_Status', 'Use_Promotion']\n",
        "# these columns need to be onehot encode or vectorise\n",
        "\n",
        "dates = ['Expected_checkin', 'Expected_checkout', 'Booking_date'] #need to feature engineer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfIlCPWsRMji"
      },
      "source": [
        "raw_df[object_cols].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h8s4Sgm4RfF"
      },
      "source": [
        "raw_df['Reservation_Status'] = raw_df['Reservation_Status'].map({'Check-In':0, 'Canceled':1, 'No-Show':2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MemKy_6rRQHK"
      },
      "source": [
        "checkin, canceled, noshow = np.bincount(raw_df['Reservation_Status'])\n",
        "total = checkin + canceled + noshow\n",
        "print('Examples:\\n    Total: {}\\n    Check-In: {} ({:.2f}% of total)\\n    Canceled: {} ({:.2f}% of total)\\n     No-Show: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, checkin, 100 * checkin / total, canceled, 100 * canceled / total, noshow, 100 * noshow / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-B0DsWD5urc"
      },
      "source": [
        "cleaned_df = raw_df.copy()\n",
        "\n",
        "#one-hot encoding\n",
        "one_hot_encoded_lst = ['Ethnicity', 'Educational_Level',\n",
        "       'Income', 'Country_region', 'Hotel_Type', \n",
        "       'Meal_Type', 'Deposit_type', 'Booking_channel'] \n",
        "cleaned_df = pd.get_dummies(cleaned_df, columns=one_hot_encoded_lst)\n",
        "\n",
        "#binary value encoding\n",
        "cleaned_df['Gender'] = cleaned_df['Gender'].map({'F':0, 'M':1})\n",
        "cleaned_df['Visted_Previously'] = cleaned_df['Visted_Previously'].map({'No':0, 'Yes':1})\n",
        "cleaned_df['Previous_Cancellations'] = cleaned_df['Previous_Cancellations'].map({'No':0, 'Yes':1})\n",
        "cleaned_df['Required_Car_Parking'] = cleaned_df['Required_Car_Parking'].map({'Yes':1, 'No':0})\n",
        "cleaned_df['Use_Promotion'] = cleaned_df['Use_Promotion'].map({'Yes':1, 'No':0})\n",
        "\n",
        "cleaned_df[dates[0]] = pd.to_datetime(cleaned_df[dates[0]])\n",
        "cleaned_df[dates[1]] = pd.to_datetime(cleaned_df[dates[1]])\n",
        "cleaned_df[dates[2]] = pd.to_datetime(cleaned_df[dates[2]])\n",
        "\n",
        "cleaned_df['Expected_stay'] = (cleaned_df[dates[1]] - cleaned_df[dates[0]]).dt.days\n",
        "cleaned_df['Booking_to_checkingin'] = (cleaned_df[dates[0]] - cleaned_df[dates[2]]).dt.days\n",
        "cleaned_df['Month_of_stay'] = cleaned_df[dates[0]].dt.month\n",
        "cleaned_df['Actual_cost'] = cleaned_df['Expected_stay'] * (cleaned_df['Room_Rate']*(100 - cleaned_df['Discount_Rate']))\n",
        "\n",
        "weekdayin = cleaned_df[dates[0]].dt.dayofweek\n",
        "weekdayout = cleaned_df[dates[1]].dt.dayofweek\n",
        "fina = []\n",
        "for x,y in zip(weekdayin, weekdayout):\n",
        "  t = []\n",
        "  if y >= x:\n",
        "    for i in range(x, y + 1):\n",
        "      t.append(i)\n",
        "    if 5 in t or 6 in t:\n",
        "      fina.append(1)\n",
        "    else:\n",
        "      fina.append(0)\n",
        "  else:\n",
        "    for i in range(x, 7):\n",
        "      t.append(i)\n",
        "    for j in range(0, y + 1):\n",
        "      t.append(i)\n",
        "    if 5 in t or 6 in t:\n",
        "      fina.append(1)\n",
        "    else:\n",
        "      fina.append(0)\n",
        "cleaned_df['weekend_stay'] = pd.DataFrame(fina,columns=['weekend_stay'])['weekend_stay'].values\n",
        "\n",
        "cleaned_df = cleaned_df.drop(dates, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO5IYKFSZ1ER"
      },
      "source": [
        "cleaned_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPuLpO-_9YLs"
      },
      "source": [
        "eps = 0.001\n",
        "cleaned_df['Log Actual_cost'] = np.log(cleaned_df.pop('Actual_cost')+eps)\n",
        "cleaned_df['Log Room_Rate'] = np.log(cleaned_df.pop('Room_Rate')+eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q87vcfdD-mx6"
      },
      "source": [
        "cleaned_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTlrzIP1F7ZJ"
      },
      "source": [
        "cleaned_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWX66nvYX27N"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJEtF2q8-re1"
      },
      "source": [
        "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
        "\n",
        "train_labels = np.array(train_df.pop('Reservation_Status'))\n",
        "bool_checkin_labels = train_labels == 0\n",
        "bool_canceled_labels = train_labels == 1\n",
        "bool_noshow_labels = train_labels == 2\n",
        "val_labels = np.array(val_df.pop('Reservation_Status'))\n",
        "test_labels = np.array(test_df.pop('Reservation_Status'))\n",
        "\n",
        "train_features = np.array(train_df)\n",
        "val_features = np.array(val_df)\n",
        "test_features = np.array(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeO__enfEfV0"
      },
      "source": [
        "train_labels = tf.keras.utils.to_categorical(train_labels, 3)\n",
        "val_labels = tf.keras.utils.to_categorical(val_labels, 3)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C848W_04_4Si"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "val_features = scaler.transform(val_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRApExtiANQO"
      },
      "source": [
        "checkin_df = pd.DataFrame(train_features[ bool_checkin_labels], columns=train_df.columns)\n",
        "canceled_df = pd.DataFrame(train_features[bool_canceled_labels], columns=train_df.columns)\n",
        "noshow_df = pd.DataFrame(train_features[bool_noshow_labels], columns=train_df.columns)\n",
        "\n",
        "sns.jointplot(checkin_df['Booking_to_checkingin'], checkin_df['Log Actual_cost'],\n",
        "              kind='hex', xlim=(-5,5), ylim=(-5,5))\n",
        "plt.suptitle(\"Check-In distribution\")\n",
        "\n",
        "sns.jointplot(canceled_df['Booking_to_checkingin'], canceled_df['Log Actual_cost'],\n",
        "              kind='hex', xlim=(-5,5), ylim=(-5,5))\n",
        "_ = plt.suptitle(\"Canceled distribution\")\n",
        "\n",
        "sns.jointplot(noshow_df['Booking_to_checkingin'], noshow_df['Log Actual_cost'],\n",
        "              kind='hex', xlim=(-5,5), ylim=(-5,5))\n",
        "_ = plt.suptitle(\"No-Show distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QGUo60oB61O"
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(3, activation='softmax',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.CategoricalCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujgEXQa1C658"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jJDA6rNC_nI"
      },
      "source": [
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM3BPVctDBoW"
      },
      "source": [
        "model.predict(train_features[:10]).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbzYF9lhDHcN"
      },
      "source": [
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
        "model.save_weights(initial_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBHZF7CGGMr0"
      },
      "source": [
        "val_features[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3AI80VDaNQ"
      },
      "source": [
        "model = make_model()\n",
        "model.load_weights(initial_weights)\n",
        "model.layers[-1].bias.assign([0.0, 0.0, 0.0])\n",
        "zero_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Cbi_unDf93"
      },
      "source": [
        "model = make_model()\n",
        "model.load_weights(initial_weights)\n",
        "careful_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUDlSN5uHXgJ"
      },
      "source": [
        "def plot_loss(history, label, n):\n",
        "  # Use a log scale on y-axis to show the wide range of values.\n",
        "  plt.semilogy(history.epoch, history.history['loss'],\n",
        "               color=colors[n], label='Train ' + label)\n",
        "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
        "               color=colors[n], label='Val ' + label,\n",
        "               linestyle=\"--\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCP2SQWYHceu"
      },
      "source": [
        "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
        "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTVQWwtgHfws"
      },
      "source": [
        "model = make_model()\n",
        "model.load_weights(initial_weights)\n",
        "baseline_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XGsu1L-IIVh"
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics = ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDL_spcxIaK7"
      },
      "source": [
        "plot_metrics(baseline_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQHdXOrMIcZ8"
      },
      "source": [
        "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVpxzIeIroR"
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels.argmax(axis=1), predictions.argmax(axis=1) > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIUp_lwaI2aO"
      },
      "source": [
        "baseline_results = model.evaluate(test_features, test_labels,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm(test_labels, test_predictions_baseline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlvc97x5JOUm"
      },
      "source": [
        "weight_for_0 = (1 / checkin)*(total)/2.0 \n",
        "weight_for_1 = (1 / canceled)*(total)/2.0\n",
        "weight_for_2 = (1 / noshow)*(total)/2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4xaohweJ6f1"
      },
      "source": [
        "weighted_model = make_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAzIvRNKBCh"
      },
      "source": [
        "plot_metrics(weighted_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvKCfJW5MOQy"
      },
      "source": [
        "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du3Yi9hmMUQ1"
      },
      "source": [
        "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
        "                                           batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm(test_labels, test_predictions_weighted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5-qfGzLMWnV"
      },
      "source": [
        "checkin_features = train_features[bool_checkin_labels]\n",
        "canceled_features = train_features[bool_canceled_labels]\n",
        "noshow_features = train_features[bool_noshow_labels]\n",
        "\n",
        "checkin_labels = train_labels[bool_checkin_labels]\n",
        "canceled_labels = train_labels[bool_canceled_labels]\n",
        "noshow_labels = train_labels[bool_noshow_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi13d9KmP5ZT"
      },
      "source": [
        "ids = np.arange(len(canceled_features))\n",
        "choices = np.random.choice(ids, len(checkin_features))\n",
        "\n",
        "res_canceled_features = canceled_features[choices]\n",
        "res_canceled_labels = canceled_labels[choices]\n",
        "\n",
        "res_canceled_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhazMLWPRKs4"
      },
      "source": [
        "ids = np.arange(len(noshow_features))\n",
        "choices = np.random.choice(ids, len(checkin_features))\n",
        "\n",
        "res_noshow_features = noshow_features[choices]\n",
        "res_noshow_labels = noshow_labels[choices]\n",
        "\n",
        "res_noshow_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBG7FJFPRYZv"
      },
      "source": [
        "resampled_features = np.concatenate([res_noshow_features, res_canceled_features, checkin_features], axis=0)\n",
        "resampled_labels = np.concatenate([res_noshow_labels, res_canceled_labels, checkin_labels], axis=0)\n",
        "\n",
        "order = np.arange(len(resampled_labels))\n",
        "np.random.shuffle(order)\n",
        "resampled_features = resampled_features[order]\n",
        "resampled_labels = resampled_labels[order]\n",
        "\n",
        "resampled_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-R8_jomSGsX"
      },
      "source": [
        "BUFFER_SIZE = 100000\n",
        "\n",
        "def make_ds(features, labels):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
        "  ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
        "  return ds\n",
        "\n",
        "checkin_ds = make_ds(checkin_features, checkin_labels)\n",
        "canceled_ds = make_ds(canceled_features, canceled_labels)\n",
        "noshow_ds = make_ds(noshow_features, noshow_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6JQVNES63c"
      },
      "source": [
        "for features, label in checkin_ds.take(1):\n",
        "  print(\"Features:\\n\", features.numpy())\n",
        "  print()\n",
        "  print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_4BBRNDS-dp"
      },
      "source": [
        "resampled_ds = tf.data.experimental.sample_from_datasets([checkin_ds, canceled_ds, noshow_ds], weights=[0.3, 0.3, 0.3])\n",
        "resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHNZxYXPTQvC"
      },
      "source": [
        "for features, label in resampled_ds.take(1):\n",
        "  print(label.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoQcMvCrTUos"
      },
      "source": [
        "resampled_steps_per_epoch = np.ceil(2.0*checkin/BATCH_SIZE)\n",
        "resampled_steps_per_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-amhdbOTaNW"
      },
      "source": [
        "resampled_model = make_model()\n",
        "resampled_model.load_weights(initial_weights)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = resampled_model.layers[-1] \n",
        "output_layer.bias.assign([0, 0, 0])\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
        "\n",
        "resampled_history = resampled_model.fit(\n",
        "    resampled_ds,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=resampled_steps_per_epoch,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=val_ds),"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrgSVFoPTwFu"
      },
      "source": [
        "resampled_model = make_model()\n",
        "resampled_model.load_weights(initial_weights)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = resampled_model.layers[-1] \n",
        "output_layer.bias.assign([0, 0, 0])\n",
        "\n",
        "resampled_history = resampled_model.fit(\n",
        "    resampled_ds,\n",
        "    # These are not real epochs\n",
        "    steps_per_epoch=20,\n",
        "    epochs=10*EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqaW56KwUHgU"
      },
      "source": [
        "plot_metrics(resampled_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl5vFsCbUWIG"
      },
      "source": [
        "train_predictions_resampled = resampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_resampled = resampled_model.predict(test_features, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b400yTz6Ud36"
      },
      "source": [
        "resampled_results = resampled_model.evaluate(test_features, test_labels,\n",
        "                                             batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_cm(test_labels, test_predictions_resampled, p=0.66)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckDhOSNGUulR"
      },
      "source": [
        "test_predictions_resampled.argmax(axis=1).tolist().count(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX_P-BEDWOxC"
      },
      "source": [
        "submission_df = pd.read_csv('/content/Hotel-A-test.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7N9aa-lWsHO"
      },
      "source": [
        "def prepare_submission_df(raw_df, one_hot_encoded_lst, dates, scaler):\n",
        "  cleaned_df = raw_df.copy()\n",
        "\n",
        "  #one-hot encoding\n",
        "  one_hot_encoded_lst = ['Ethnicity', 'Educational_Level',\n",
        "        'Income', 'Country_region', 'Hotel_Type', \n",
        "        'Meal_Type', 'Deposit_type', 'Booking_channel'] \n",
        "  cleaned_df = pd.get_dummies(cleaned_df, columns=one_hot_encoded_lst)\n",
        "\n",
        "  #binary value encoding\n",
        "  cleaned_df['Gender'] = cleaned_df['Gender'].map({'F':0, 'M':1})\n",
        "  cleaned_df['Visted_Previously'] = cleaned_df['Visted_Previously'].map({'No':0, 'Yes':1})\n",
        "  cleaned_df['Previous_Cancellations'] = cleaned_df['Previous_Cancellations'].map({'No':0, 'Yes':1})\n",
        "  cleaned_df['Required_Car_Parking'] = cleaned_df['Required_Car_Parking'].map({'Yes':1, 'No':0})\n",
        "  cleaned_df['Use_Promotion'] = cleaned_df['Use_Promotion'].map({'Yes':1, 'No':0})\n",
        "\n",
        "  cleaned_df[dates[0]] = pd.to_datetime(cleaned_df[dates[0]])\n",
        "  cleaned_df[dates[1]] = pd.to_datetime(cleaned_df[dates[1]])\n",
        "  cleaned_df[dates[2]] = pd.to_datetime(cleaned_df[dates[2]])\n",
        "\n",
        "  cleaned_df['Expected_stay'] = (cleaned_df[dates[1]] - cleaned_df[dates[0]]).dt.days\n",
        "  cleaned_df['Booking_to_checkingin'] = (cleaned_df[dates[0]] - cleaned_df[dates[2]]).dt.days\n",
        "  cleaned_df['Month_of_stay'] = cleaned_df[dates[0]].dt.month\n",
        "  cleaned_df['Actual_cost'] = cleaned_df['Expected_stay'] * (cleaned_df['Room_Rate']*(100 - cleaned_df['Discount_Rate']))\n",
        "\n",
        "  weekdayin = cleaned_df[dates[0]].dt.dayofweek\n",
        "  weekdayout = cleaned_df[dates[1]].dt.dayofweek\n",
        "  fina = []\n",
        "  for x,y in zip(weekdayin, weekdayout):\n",
        "    t = []\n",
        "    if y >= x:\n",
        "      for i in range(x, y + 1):\n",
        "        t.append(i)\n",
        "      if 5 in t or 6 in t:\n",
        "        fina.append(1)\n",
        "      else:\n",
        "        fina.append(0)\n",
        "    else:\n",
        "      for i in range(x, 7):\n",
        "        t.append(i)\n",
        "      for j in range(0, y + 1):\n",
        "        t.append(i)\n",
        "      if 5 in t or 6 in t:\n",
        "        fina.append(1)\n",
        "      else:\n",
        "        fina.append(0)\n",
        "  cleaned_df['weekend_stay'] = pd.DataFrame(fina,columns=['weekend_stay'])['weekend_stay'].values\n",
        "  cleaned_df = cleaned_df.drop(dates, 1)\n",
        "\n",
        "  eps = 0.001\n",
        "  cleaned_df['Log Actual_cost'] = np.log(cleaned_df.pop('Actual_cost')+eps)\n",
        "  cleaned_df['Log Room_Rate'] = np.log(cleaned_df.pop('Room_Rate')+eps)\n",
        "\n",
        "  submission_features = scaler.transform(np.array(cleaned_df))\n",
        "  submission_features = np.clip(submission_features, -5, 5)\n",
        "\n",
        "  return submission_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h4M8P92WfhR"
      },
      "source": [
        "submission_features = prepare_submission_df(submission_df, one_hot_encoded_lst, dates, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHIwWZJnY_ca"
      },
      "source": [
        "def print_counts(pred):\n",
        "  checkins = pred.argmax(axis = 1).tolist().count(0)\n",
        "  canceled = pred.argmax(axis = 1).tolist().count(1)\n",
        "  noshow = pred.argmax(axis = 1).tolist().count(2)\n",
        "  print(f\"Check-In Count :  {checkins}\\nCanceled Count :  {canceled}\\nNo Show Count :  {noshow}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ty8EN9Ra23X"
      },
      "source": [
        "baseline_submission_pred = model.predict(submission_features, batch_size=BATCH_SIZE)\n",
        "print_counts(baseline_submission_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBNkHEuGaf-F"
      },
      "source": [
        "weighted_submission_pred = weighted_model.predict(submission_features, batch_size=BATCH_SIZE)\n",
        "print_counts(weighted_submission_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezZGT122YvwN"
      },
      "source": [
        "resampled_submission_pred = resampled_model.predict(submission_features, batch_size=BATCH_SIZE)\n",
        "print_counts(resampled_submission_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3RMXsOSamhb"
      },
      "source": [
        "def create_submission_df(pred, df):\n",
        "  col_drop = df.columns.tolist()\n",
        "  submission = df.drop(col_drop, 1)\n",
        "  predictions = pred.argmax(axis = 1).tolist()\n",
        "  predictions = [x+1 for x in predictions]\n",
        "  submission['Reservation_status'] = pd.DataFrame(predictions,columns=['Reservation_status'])['Reservation_status'].values\n",
        "  return submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg8Ct48LcRv_"
      },
      "source": [
        "create_submission_df(resampled_submission_pred, submission_df).to_csv('submission2-day2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4zOWIIsdngy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}